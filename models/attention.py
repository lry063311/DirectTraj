import torch
from torch import nn
from torch.nn import functional as F
from einops import rearrange
import math


class VanillaAttention(nn.Module):
    def __init__(
            self,
            dim: int,
            heads: int,
            dim_head: int
    ):
        super().__init__()
        self.inner_dim = dim_head * heads
        self.heads = heads
        self.head_dim = dim_head
        # ä¿å­˜ç¼©æ”¾å› å­ï¼Œç”¨äºæ‰‹åŠ¨è®¡ç®—
        self.scale = dim_head ** -0.5

        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias=False)
        self.to_out = nn.Linear(self.inner_dim, dim)

    def forward(self, x: torch.Tensor):
        B, L, D = x.shape

        q, k, v = self.to_qkv(x).chunk(3, dim=-1)

        # ä¿æŒæ‚¨ä¹ æƒ¯çš„ einops å†™æ³•
        q = rearrange(q, "B L (h d) -> B h L d", h=self.heads)
        k = rearrange(k, "B L (h d) -> B h L d", h=self.heads)
        v = rearrange(v, "B L (h d) -> B h L d", h=self.heads)

        # ğŸ”¥ğŸ”¥ğŸ”¥ å…¼å®¹æ€§æ ¸å¿ƒä¿®æ”¹ ğŸ”¥ğŸ”¥ğŸ”¥
        # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦å­˜åœ¨ PyTorch 2.0 çš„é«˜æ•ˆç®—å­
        if hasattr(F, 'scaled_dot_product_attention'):
            # å¦‚æœæœ‰ï¼Œç›´æ¥ç”¨ï¼ˆå’Œæ‚¨åŸä»£ç å®Œå…¨ä¸€è‡´ï¼‰
            x = F.scaled_dot_product_attention(query=q, key=k, value=v, is_causal=False)
        else:
            # å¦‚æœæ²¡æœ‰ï¼ˆæ—§ç‰ˆæœ¬ PyTorchï¼‰ï¼Œä½¿ç”¨å®Œå…¨ç­‰ä»·çš„æ•°å­¦å…¬å¼æ‰‹åŠ¨è®¡ç®—
            # 1. è®¡ç®— Q * K^T / scale
            # q: [B, h, L, d], k.transpose: [B, h, d, L] -> scores: [B, h, L, L]
            dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale

            # 2. Softmax å½’ä¸€åŒ–
            attn = F.softmax(dots, dim=-1)

            # 3. ä¹˜ V
            # attn: [B, h, L, L], v: [B, h, L, d] -> x: [B, h, L, d]
            x = torch.matmul(attn, v)

        # è¿˜åŸå½¢çŠ¶
        x = rearrange(x, "B h L d -> B L (h d)")
        # è¿™ä¸€æ­¥é€šå¸¸æ²¡å¿…è¦ï¼Œä½†ä¸ºäº†å’Œæ‚¨åŸä»£ç  100% ä¿æŒä¸€è‡´ä¿ç•™ä¸‹æ¥
        x = x.to(q.dtype)

        # linear proj
        x = self.to_out(x)
        return x